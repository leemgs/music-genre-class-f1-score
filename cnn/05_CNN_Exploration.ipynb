{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import utils\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "mel_specs = pd.read_csv('../data/genre_mel_specs_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>84472</th>\n",
       "      <th>84473</th>\n",
       "      <th>84474</th>\n",
       "      <th>84475</th>\n",
       "      <th>84476</th>\n",
       "      <th>84477</th>\n",
       "      <th>84478</th>\n",
       "      <th>84479</th>\n",
       "      <th>labels</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-38.714360</td>\n",
       "      <td>-33.474228</td>\n",
       "      <td>-27.310455</td>\n",
       "      <td>-25.299803</td>\n",
       "      <td>-28.430004</td>\n",
       "      <td>-28.678144</td>\n",
       "      <td>-27.830578</td>\n",
       "      <td>-26.894180</td>\n",
       "      <td>-34.463097</td>\n",
       "      <td>-30.501217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>classical</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-21.302162</td>\n",
       "      <td>-39.085693</td>\n",
       "      <td>-28.659452</td>\n",
       "      <td>-31.364570</td>\n",
       "      <td>-30.419193</td>\n",
       "      <td>-40.327023</td>\n",
       "      <td>-28.706080</td>\n",
       "      <td>-43.529984</td>\n",
       "      <td>-33.345123</td>\n",
       "      <td>-33.197315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rock</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-15.267654</td>\n",
       "      <td>-14.026318</td>\n",
       "      <td>-14.920742</td>\n",
       "      <td>-16.219590</td>\n",
       "      <td>-16.906425</td>\n",
       "      <td>-20.542664</td>\n",
       "      <td>-25.683270</td>\n",
       "      <td>-10.716038</td>\n",
       "      <td>-21.445236</td>\n",
       "      <td>-18.547516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pop</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-31.311068</td>\n",
       "      <td>-36.689530</td>\n",
       "      <td>-42.981520</td>\n",
       "      <td>-38.595932</td>\n",
       "      <td>-35.907497</td>\n",
       "      <td>-39.644302</td>\n",
       "      <td>-43.886433</td>\n",
       "      <td>-42.308525</td>\n",
       "      <td>-35.456673</td>\n",
       "      <td>-33.849125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>jazz</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-18.864574</td>\n",
       "      <td>-22.681887</td>\n",
       "      <td>-24.525406</td>\n",
       "      <td>-29.330860</td>\n",
       "      <td>-28.273998</td>\n",
       "      <td>-29.353290</td>\n",
       "      <td>-31.516180</td>\n",
       "      <td>-25.657170</td>\n",
       "      <td>-27.893257</td>\n",
       "      <td>-30.826773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>metal</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 84482 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -38.714360 -33.474228 -27.310455 -25.299803 -28.430004 -28.678144   \n",
       "1 -21.302162 -39.085693 -28.659452 -31.364570 -30.419193 -40.327023   \n",
       "2 -15.267654 -14.026318 -14.920742 -16.219590 -16.906425 -20.542664   \n",
       "3 -31.311068 -36.689530 -42.981520 -38.595932 -35.907497 -39.644302   \n",
       "4 -18.864574 -22.681887 -24.525406 -29.330860 -28.273998 -29.353290   \n",
       "\n",
       "           6          7          8          9  ...  84472  84473  84474  \\\n",
       "0 -27.830578 -26.894180 -34.463097 -30.501217  ...    0.0    0.0    0.0   \n",
       "1 -28.706080 -43.529984 -33.345123 -33.197315  ...    0.0    0.0    0.0   \n",
       "2 -25.683270 -10.716038 -21.445236 -18.547516  ...    0.0    0.0    0.0   \n",
       "3 -43.886433 -42.308525 -35.456673 -33.849125  ...    0.0    0.0    0.0   \n",
       "4 -31.516180 -25.657170 -27.893257 -30.826773  ...    0.0    0.0    0.0   \n",
       "\n",
       "   84475  84476  84477  84478  84479     labels   y  \n",
       "0    0.0    0.0    0.0    0.0    0.0  classical   2  \n",
       "1    0.0    0.0    0.0    0.0    0.0       rock  10  \n",
       "2    0.0    0.0    0.0    0.0    0.0        pop   8  \n",
       "3    0.0    0.0    0.0    0.0    0.0       jazz   6  \n",
       "4    0.0    0.0    0.0    0.0    0.0      metal   7  \n",
       "\n",
       "[5 rows x 84482 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 rows for reference\n",
    "mel_specs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Function to Get a Subset of the Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre_subset(data, genre_subset):\n",
    "    '''\n",
    "    This function takes in a dataframe and a list of genres and returns a new dataframe only including\n",
    "    the genres in the given list. Its index is reset and new labels are created so that the labels are 0 \n",
    "    through one less than the number of genres. \n",
    "    '''\n",
    "    \n",
    "    # Filtering the dataframe for the subset of the genres and resetting the index\n",
    "    df = data.loc[data['labels'].isin(genre_subset)]\n",
    "    df = df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    # Creating a new label dictionary\n",
    "    new_label_dict = {}\n",
    "    for i in range(len(genre_subset)):\n",
    "        new_label_dict[genre_subset[i]] = i\n",
    "    \n",
    "    # Changing labels to be the new labels\n",
    "    df['y'] = df['labels'].map(new_label_dict)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Preprocess the Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mel_spec_data(data, genre_subset):\n",
    "    '''\n",
    "    This function takes in a dataframe of audio files and a list of genres,\n",
    "    calls the function get_genre_subset to get a dataframe including only the given genres,\n",
    "    and completes all of the data preprocessing steps needed to run a neural network.\n",
    "    \n",
    "    Preprecessing steps include:\n",
    "    1. Reshaping the mel spectrograms to their original form (128 x 660)\n",
    "    2. Defining the array of targets\n",
    "    3. Train test split\n",
    "    4. Standardizing the data\n",
    "    5. Reshaping the data to be 128 x 660 x 1, where the 1 represents a single color channel\n",
    "    6. One-hot-encoding target data\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): a dataframe of audio files, flattened mel spectrograms, and genre labels\n",
    "    genre_subset (list): a list of genres included in the dataframe\n",
    "    \n",
    "    Returns:\n",
    "    X_train (array): training set of features\n",
    "    X_test (array): testing set of features\n",
    "    y_train (array): training set of targets\n",
    "    y_test (array): testing set of targets\n",
    "    '''\n",
    "    \n",
    "    # Getting a subset of the genres using our genre_subset function\n",
    "    subset = get_genre_subset(data, genre_subset)\n",
    "    \n",
    "    # Dropping label columns to prepare our feature vector\n",
    "    specs = subset.drop(columns=['labels', 'y'])\n",
    "    \n",
    "    # Reshaping the arrays to their original \"image\" form\n",
    "    X = []\n",
    "    for i in range(len(genre_subset)*100):\n",
    "        X.append(np.array(specs.iloc[i]).reshape(128,660))\n",
    "        \n",
    "    # Converting list X to an array\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Defining our targets\n",
    "    y = subset.loc[subset['labels'].isin(genre_subset), 'y'].values\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)\n",
    "    \n",
    "    # Scaling our data to be between 0 and 1\n",
    "    X_train /= -80\n",
    "    X_test /= -80\n",
    "    \n",
    "    # Reshaping images to be 128 x 660 x 1\n",
    "    X_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 128, 660, 1)\n",
    "    \n",
    "    # One hot encoding our labels\n",
    "    y_train = to_categorical(y_train, len(genre_subset))\n",
    "    y_test = to_categorical(y_test, len(genre_subset))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the genres\n",
    "genre_list = {\n",
    "    'classical': 0,\n",
    "    'hiphop': 1,\n",
    "    'jazz': 2,\n",
    "    'metal': 3,\n",
    "    'pop': 4,\n",
    "    'rock': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "genre_subset = [\n",
    "    'hiphop',\n",
    "    'jazz',\n",
    "    'metal',\n",
    "    'pop'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our function to get our features and targets\n",
    "X_train, X_test, y_train, y_test = preprocess_mel_spec_data(mel_specs, genre_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model for Subset of Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (320, 4) was passed for an output of shape (None, 7) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-296a209a3097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         epochs=15)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (320, 4) was passed for an output of shape (None, 7) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "np.random.seed(23456)\n",
    "\n",
    "# Initiating an empty neural network\n",
    "cnn_model = Sequential(name='cnn_1')\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128,660,1)))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding a flattened layer to input our image data\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Adding a dense layer with 64 neurons\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adding a dropout layer for regularization\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "# Adding an output layer\n",
    "cnn_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Compiling our neural network\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(1,11), range(1,11))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the cnn model\n",
    "predictions = cnn_model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix \n",
    "# row: actual\n",
    "# columns: predicted\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions, 1))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the confusion matrix with labels for readability \n",
    "confusion_df = pd.DataFrame(conf_matrix)\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "genre_subset = [\n",
    "    0:'hiphop',\n",
    "    1:'jazz',\n",
    "    2:'meta',\n",
    "    3:'pop'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming rows and columns with labes\n",
    "confusion_df = confusion_df.rename(columns=genre_labels)\n",
    "confusion_df.index = confusion_df.columns\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model for Binary Classification of Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "genre_subset_2 = [\n",
    "    'metal',\n",
    "    'classical'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our function to get our features and targets\n",
    "X_train, X_test, y_train, y_test = preprocess_mel_spec_data(mel_specs, genre_subset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23456)\n",
    "\n",
    "# Initiating an empty neural network\n",
    "cnn_model_2 = Sequential(name='cnn_2')\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model_2.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128,660,1)))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model_2.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding a flattened layer to input our image data\n",
    "cnn_model_2.add(Flatten())\n",
    "\n",
    "# Adding a dense layer with 64 neurons\n",
    "cnn_model_2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adding a dropout layer for regularization\n",
    "cnn_model_2.add(Dropout(0.25))\n",
    "\n",
    "# Adding an output layer\n",
    "cnn_model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compiling our neural network\n",
    "cnn_model_2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = cnn_model_2.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the model summary\n",
    "cnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(1,11), range(1,11))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the cnn model\n",
    "predictions_2 = cnn_model_2.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix \n",
    "# row: actual\n",
    "# columns: predicted\n",
    "conf_matrix_2 = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions_2, 1))\n",
    "conf_matrix_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the confusion matrix with labels for readability \n",
    "confusion_df_2 = pd.DataFrame(conf_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "genre_labels_2 = {\n",
    "    0:'metal',\n",
    "    1:'classical'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming rows and columns with labes\n",
    "confusion_df_2 = confusion_df_2.rename(columns=genre_labels_2)\n",
    "confusion_df_2.index = confusion_df_2.columns\n",
    "confusion_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
